{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import itertools as iter\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "\n",
    "\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "print(DATA_PATH)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3544f8a",
   "metadata": {},
   "source": [
    "## Reading tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filpaths=[]\n",
    "for root,dirs,files in os.walk(os.path.join(DATA_PATH, \"monet_tfrec\")):\n",
    "    for f in files:\n",
    "       filpaths.append(os.path.join(root,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76779a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [256, 256]\n",
    "\n",
    "def decode_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    tfrecord_format = {\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example['image'])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd1909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40646480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasetMonet = load_dataset(filpaths, labeled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bbf0d",
   "metadata": {},
   "source": [
    "## Larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfc6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles=[]\n",
    "for root, dirs, files in os.walk(os.path.join(DATA_PATH, \"processed\")):\n",
    "    for f in files:\n",
    "        allFiles.append(os.path.join(root, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    for i,file in enumerate(allFiles):\n",
    "        image = tf.io.read_file(file)\n",
    "        image = tf.io.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.random_brightness(image, 0.1)\n",
    "        image = tf.image.random_contrast(image, 0.95, 1.05)\n",
    "        image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
    "        yield image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d109eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = generator()\n",
    "test = next(g)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c19d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(g)\n",
    "plt.imshow(((test.numpy()+1)/2))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(generator, output_signature=(tf.TensorSpec(shape=(*IMAGE_SIZE, 3), dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143ffe5",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a554f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98752d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def upscaleBlock(xIn, channelsBefore, channelsAfter, filtersize):\n",
    "    x=tf.keras.layers.Conv2D(channelsBefore, filtersize, padding=\"same\")(xIn)\n",
    "    x=tf.keras.layers.BatchNormalization()(x)\n",
    "    x=tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(channelsAfter, filtersize, padding=\"same\", kernel_regularizer=tf.keras.regularizers.L2(l2=0.01))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x=tf.keras.layers.LeakyReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def createGenerator():\n",
    "    inputs = tf.keras.Input(shape=(1024,))\n",
    "    # x=layers.Embedding(VOCAB_SIZE, EMBED_DIM, name=\"embedding\")(inputs)\n",
    "\n",
    "    x=tf.keras.layers.Dense(4*4*32, kernel_regularizer=\"l2\")(inputs)\n",
    "    x=tf.keras.layers.BatchNormalization()(x)\n",
    "    x=tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x=tf.keras.layers.Reshape((4,4,32))(x)\n",
    "\n",
    "    x = upscaleBlock(x, 32, 32, 3)\n",
    "\n",
    "    x = upscaleBlock(x, 32, 16, 3)\n",
    "\n",
    "    x = upscaleBlock(x, 16, 8, 3)\n",
    "\n",
    "    x = upscaleBlock(x, 8, 8, 3)\n",
    "\n",
    "    x = upscaleBlock(x, 8, 4, 3)\n",
    "\n",
    "    x = upscaleBlock(x, 4, 2, 3)\n",
    "    \n",
    "    x=tf.keras.layers.Conv2D(3, 5, padding=\"same\", kernel_regularizer=tf.keras.regularizers.L2(l2=0.01))(x)\n",
    "    outputs = tf.keras.activations.tanh(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"generator\")\n",
    "\n",
    "generator = createGenerator()\n",
    "\n",
    "generator.summary()\n",
    "\n",
    "dot_img_file = './'+generator.name +'.png'\n",
    "tf.keras.utils.plot_model(generator, to_file=dot_img_file, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d56de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downscaleBlock(xIn, channelsBefore, channelsAfter, filtersize):\n",
    "    x = tf.keras.layers.Conv2D(channelsBefore, filtersize, kernel_regularizer=tf.keras.regularizers.L2(l2=0.01))(xIn)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(channelsAfter, filtersize, strides=2, kernel_regularizer=tf.keras.regularizers.L2(l2=0.01))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def createDiscriminator():\n",
    "    inputs = tf.keras.Input(shape=(*IMAGE_SIZE, 3))\n",
    "\n",
    "\n",
    "    x = downscaleBlock(inputs, 2, 4, 3)\n",
    "\n",
    "    x = downscaleBlock(x, 4, 8, 3)\n",
    "    \n",
    "    x = downscaleBlock(x, 8, 16, 3)\n",
    "    \n",
    "    x = downscaleBlock(x, 16, 32, 3)\n",
    "\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32,3, kernel_regularizer=tf.keras.regularizers.L2(l2=0.01))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(64, kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(64, kernel_regularizer=tf.keras.regularizers.L2(l2=0.01))(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    # outputs=layers.Dense(1, activation=tf.keras.activations.sigmoid)(x)\n",
    "    outputs=layers.Dense(1,)(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"Discriminator\")\n",
    "\n",
    "discriminator = createDiscriminator()\n",
    "\n",
    "discriminator.summary()\n",
    "\n",
    "dot_img_file = './'+discriminator.name +'.png'\n",
    "tf.keras.utils.plot_model(discriminator, to_file=dot_img_file, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6afa90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadExistingModel=False\n",
    "\n",
    "if loadExistingModel:\n",
    "    generator = tf.keras.models.load_model(os.path.join(DATA_PATH, \"generator_epoch_195\"))\n",
    "    discriminator = tf.keras.models.load_model(os.path.join(DATA_PATH, \"discriminator_epoch_195\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1533cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveImages(model, epoch):\n",
    "  testInput = tf.random.uniform((9, 1024))\n",
    "  predictions = model(testInput, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(np.rint(predictions[i, :, :, :] * 127.5 + 127.5).astype(int))\n",
    "    plt.axis('off')\n",
    "\n",
    "  plt.savefig(os.path.join(DATA_PATH,\"trainImages\", 'epoch_{:04d}.png'.format(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9113c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE=64\n",
    "LOG_INTERVAL=5\n",
    "epochs = 200\n",
    "saveModel=True\n",
    "\n",
    "startEpoch=0\n",
    "\n",
    "Td=2\n",
    "Tg=3\n",
    "\n",
    "log_dir = \"./logs/\"+generator.name+\"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1,\n",
    "                                                      write_graph=True, update_freq=5)\n",
    "\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# Instantiate an optimizer .\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "optimizerGen = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "optimizerDis = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# Instantiate a loss function.\n",
    "lossFnGen = tf.keras.losses.MeanSquaredError()\n",
    "# lossFnGen = tf.keras.losses.BinaryCrossentropy()\n",
    "lossFnDis = tf.keras.losses.MeanSquaredError()\n",
    "# lossFnDis = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "accuracyDis = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "# batchedDataset = datasetMonet.batch(BATCH_SIZE, drop_remainder=False)\n",
    "datasetShuffled = dataset.shuffle(200)\n",
    "batchedDataset = datasetShuffled.batch(BATCH_SIZE, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def trainStepGen(trueImages):\n",
    "    input = tf.random.uniform((BATCH_SIZE, 1024))\n",
    "    with tf.GradientTape() as tapeGen, tf.GradientTape() as tapeDis:\n",
    "        fakeImages = generator(input, training=True) \n",
    "\n",
    "        discOutputFake = discriminator(fakeImages, training=True)\n",
    "        discOutputTrue = discriminator(trueImages, training=True)\n",
    "\n",
    "        lossGen = lossFnGen(tf.ones_like(discOutputFake), discOutputFake)\n",
    "\n",
    "        lossDisTrue = lossFnDis(tf.ones_like(discOutputTrue), discOutputTrue)\n",
    "        lossDisFake = lossFnDis(tf.zeros_like(discOutputFake), discOutputFake)\n",
    "        totalLossDis = lossDisTrue + lossDisFake\n",
    "\n",
    "    gradsGen = tapeGen.gradient(lossGen, generator.trainable_weights)\n",
    "\n",
    "    accuracyDis.update_state(tf.zeros_like(discOutputFake), discOutputFake)\n",
    "    accuracyDis.update_state(tf.ones_like(discOutputTrue), discOutputTrue)\n",
    "\n",
    "    optimizerGen.apply_gradients(zip(gradsGen, generator.trainable_weights)) \n",
    "\n",
    "\n",
    "    return lossGen, totalLossDis\n",
    "\n",
    "@tf.function()\n",
    "def trainStepDis(trueImages):\n",
    "    input = tf.random.uniform((BATCH_SIZE, 1024))\n",
    "    with tf.GradientTape() as tapeGen, tf.GradientTape() as tapeDis:\n",
    "        fakeImages = generator(input, training=True) \n",
    "\n",
    "        discOutputFake = discriminator(fakeImages, training=True)\n",
    "        discOutputTrue = discriminator(trueImages, training=True)\n",
    "\n",
    "        lossGen = lossFnGen(tf.ones_like(discOutputFake), discOutputFake)\n",
    "\n",
    "        lossDisTrue = lossFnDis(tf.ones_like(discOutputTrue), discOutputTrue)\n",
    "        lossDisFake = lossFnDis(tf.zeros_like(discOutputFake), discOutputFake)\n",
    "        totalLossDis = lossDisTrue + lossDisFake\n",
    "\n",
    "    gradsDis = tapeDis.gradient(totalLossDis, discriminator.trainable_weights)\n",
    "\n",
    "    accuracyDis.update_state(tf.zeros_like(discOutputFake), discOutputFake)\n",
    "    accuracyDis.update_state(tf.ones_like(discOutputTrue), discOutputTrue)\n",
    "\n",
    "    optimizerDis.apply_gradients(zip(gradsDis, discriminator.trainable_weights)) \n",
    "\n",
    "    return lossGen, totalLossDis\n",
    "\n",
    "@tf.function()\n",
    "def trainStepGenDis(trueImages):\n",
    "    input = tf.random.uniform((BATCH_SIZE, 1024))\n",
    "    with tf.GradientTape() as tapeGen, tf.GradientTape() as tapeDis:\n",
    "        fakeImages = generator(input, training=True) \n",
    "\n",
    "        discOutputFake = discriminator(fakeImages, training=True)\n",
    "        discOutputTrue = discriminator(trueImages, training=True)\n",
    "\n",
    "        lossGen = lossFnGen(tf.ones_like(discOutputFake), discOutputFake)\n",
    "\n",
    "        lossDisTrue = lossFnDis(tf.ones_like(discOutputTrue), discOutputTrue)\n",
    "        lossDisFake = lossFnDis(tf.zeros_like(discOutputFake), discOutputFake)\n",
    "        totalLossDis = lossDisTrue + lossDisFake\n",
    "\n",
    "    gradsGen = tapeGen.gradient(lossGen, generator.trainable_weights)\n",
    "    gradsDis = tapeDis.gradient(totalLossDis, discriminator.trainable_weights)\n",
    "\n",
    "    accuracyDis.update_state(tf.zeros_like(discOutputFake), discOutputFake)\n",
    "    accuracyDis.update_state(tf.ones_like(discOutputTrue), discOutputTrue)\n",
    "\n",
    "    optimizerGen.apply_gradients(zip(gradsGen, generator.trainable_weights)) \n",
    "    optimizerDis.apply_gradients(zip(gradsDis, discriminator.trainable_weights)) \n",
    "\n",
    "    return lossGen, totalLossDis\n",
    "\n",
    "\n",
    "\n",
    "maxStep=len(allFiles)//BATCH_SIZE\n",
    "# Train the discriminator only every discHandicap steps\n",
    "discHandicap = 1\n",
    "\n",
    "for epoch in np.arange(startEpoch, startEpoch+epochs, 1):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(batchedDataset):\n",
    "        if epoch<Td:\n",
    "            lossGen, lossDis = trainStepDis(x_batch_train)\n",
    "        elif epoch<Td+Tg:\n",
    "            trainGen=True\n",
    "            trainDis=False\n",
    "            lossGen, lossDis = trainStepGen(x_batch_train)\n",
    "        else:\n",
    "            if step%discHandicap==0:\n",
    "                lossGen, lossDis = trainStepGenDis(x_batch_train)\n",
    "            else:\n",
    "                lossGen, lossDis = trainStepGen(x_batch_train)\n",
    "\n",
    "\n",
    "        # Log \n",
    "        if step % LOG_INTERVAL == 0:\n",
    "            accDis = accuracyDis.result().numpy()\n",
    "            template = 'Epoch {}/Step {}, Loss Generator: {:.4f}, Loss Discriminator: {:.4f}, Accuracy Dis: {:.4f}'\n",
    "            print(template.format(epoch, step, lossGen.numpy(), lossDis.numpy(),  accDis))\n",
    "            \n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('lossGen', lossGen, step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('lossDis', lossDis, step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('Disc Accuracy', accuracyDis.result().numpy(), step=maxStep*epoch+step)\n",
    "                summary_writer.flush()\n",
    "\n",
    "    maxStep=step\n",
    "\n",
    "    # Adaptive disc handicap\n",
    "    if accDis>0.95 and discHandicap<10 and epoch>=Td+Tg:\n",
    "        discHandicap += 1\n",
    "        print(\"Decrease disc training frequency to every {} steps\".format(discHandicap))\n",
    "    if accDis<0.65 and discHandicap>1 and epoch>=Td+Tg:\n",
    "        discHandicap -= 1\n",
    "        print(\"Increase disc training frequency to every {} steps\".format(discHandicap))\n",
    "\n",
    "    accuracyDis.reset_state()\n",
    "\n",
    "    print(\"Saving images\")\n",
    "    saveImages(generator, epoch)\n",
    "\n",
    "    if saveModel and epoch%10==0:\n",
    "      generator.save(os.path.join(DATA_PATH, \"generator_\"+\"epoch_{}\".format(epoch)))\n",
    "      discriminator.save(os.path.join(DATA_PATH, \"discriminator_\"+\"epoch_{}\".format(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "testInput = tf.random.uniform((9, 1024))\n",
    "predictions = generator(testInput, training=False)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(np.rint(predictions[i, :, :, :] * 127.5 + 127.5).astype(int))\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8af0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(generator, os.path.join(DATA_PATH, \"generator\"))\n",
    "tf.keras.models.save_model(discriminator, os.path.join(DATA_PATH, \"discriminator\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
