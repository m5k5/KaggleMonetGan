{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "\n",
    "\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "print(DATA_PATH)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3544f8a",
   "metadata": {},
   "source": [
    "## Reading tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filpaths=[]\n",
    "for root,dirs,files in os.walk(os.path.join(DATA_PATH, \"monet_tfrec\")):\n",
    "    for f in files:\n",
    "       filpaths.append(os.path.join(root,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76779a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [256, 256]\n",
    "\n",
    "def decode_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    tfrecord_format = {\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example['image'])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd1909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40646480",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetMonet = load_dataset(filpaths, labeled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143ffe5",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a554f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98752d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def createGenerator():\n",
    "    inputs = tf.keras.Input(shape=(1024,))\n",
    "    # x=layers.Embedding(VOCAB_SIZE, EMBED_DIM, name=\"embedding\")(inputs)\n",
    "\n",
    "    # x=layers.Dense(16*16*4)(inputs)\n",
    "    # x=layers.LeakyReLU()(x)\n",
    "    # x=layers.BatchNormalization()(x)\n",
    "    x=layers.Reshape((16,16,4))(inputs)\n",
    "\n",
    "    x=layers.Conv2DTranspose(8, 5, padding=\"same\")(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "    \n",
    "    x=layers.Conv2DTranspose(16, 5, strides=(2,2), padding=\"same\")(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x=layers.Conv2DTranspose(16, 5, padding=\"same\")(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "    \n",
    "    x=layers.Conv2DTranspose(32, 5, strides=(2,2), padding=\"same\")(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x=layers.Conv2DTranspose(32, 5, padding=\"same\")(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "    \n",
    "    x=layers.Conv2DTranspose(64, 5, strides=(2,2), padding=\"same\")(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x=layers.Conv2DTranspose(64, 5, padding=\"same\")(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "\n",
    "    x=layers.Conv2DTranspose(64, 5, strides=(2,2), padding=\"same\")(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x=layers.Conv2DTranspose(64, 5, padding=\"same\")(x)\n",
    "    x=layers.LeakyReLU()(x)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "    \n",
    "    x=layers.Conv2DTranspose(3, 5, padding=\"same\")(x)\n",
    "    outputs = tf.keras.activations.tanh(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"generator\")\n",
    "\n",
    "generator = createGenerator()\n",
    "\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d56de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def createDiscriminator():\n",
    "    inputs = tf.keras.Input(shape=(*IMAGE_SIZE, 3))\n",
    "    # x=layers.Embedding(VOCAB_SIZE, EMBED_DIM, name=\"embedding\")(inputs)\n",
    "    # x = layers.BatchNormalization()(tf.expand_dims(inputs,2))\n",
    "\n",
    "    x = layers.Conv2D(8,5)(inputs)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(8,5)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(16,5, strides=2)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(16,5)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(32,5, strides=2)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(32,5)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(32,5, strides=2)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(32,5)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(32,5)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(32,5)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    x=layers.Dense(32)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    x=layers.Dense(32)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    outputs=layers.Dense(1, activation=tf.keras.activations.sigmoid)(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"Discriminator\")\n",
    "\n",
    "discriminator = createDiscriminator()\n",
    "\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1533cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveImages(model, epoch):\n",
    "  testInput = tf.random.uniform((9, 1024))\n",
    "  predictions = model(testInput, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(np.rint(predictions[i, :, :, :] * 127.5 + 127.5).astype(int))\n",
    "    plt.axis('off')\n",
    "\n",
    "  plt.savefig(os.path.join(DATA_PATH,\"trainImages\", 'epoch_{:04d}.png'.format(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9113c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE=16\n",
    "LOG_INTERVAL=5\n",
    "epochs = 15\n",
    "saveModel=False\n",
    "\n",
    "\n",
    "log_dir = \"./logs/\"+generator.name+\"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1,\n",
    "                                                      write_graph=True, update_freq=5)\n",
    "\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# Instantiate an optimizer .\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "optimizerGen = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "optimizerDis = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Instantiate a loss function.\n",
    "lossFnGen = tf.keras.losses.BinaryCrossentropy()\n",
    "lossFnDis = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "accuracyDis = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "batchedDataset = datasetMonet.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "@tf.function()\n",
    "def trainStepGen(trueImages, trainGen=True, trainDis=True):\n",
    "    input = tf.random.uniform((BATCH_SIZE, 1024))\n",
    "    with tf.GradientTape() as tapeGen, tf.GradientTape() as tapeDis:\n",
    "        fakeImages = generator(input, training=True) \n",
    "\n",
    "        discOutputFake = discriminator(fakeImages, training=True)\n",
    "        discOutputTrue = discriminator(trueImages, training=True)\n",
    "\n",
    "        lossGen = lossFnGen(tf.ones_like(discOutputFake), discOutputFake)\n",
    "\n",
    "        lossDisTrue = lossFnDis(tf.ones_like(discOutputTrue), discOutputTrue)\n",
    "        lossDisFake = lossFnDis(tf.zeros_like(discOutputFake), discOutputFake)\n",
    "        totalLossDis = lossDisTrue + lossDisFake\n",
    "\n",
    "    gradsGen = tapeGen.gradient(lossGen, generator.trainable_weights)\n",
    "    gradsDis = tapeDis.gradient(totalLossDis, discriminator.trainable_weights)\n",
    "\n",
    "    accuracyDis.update_state(tf.zeros_like(discOutputFake), discOutputFake)\n",
    "    accuracyDis.update_state(tf.ones_like(discOutputTrue), discOutputTrue)\n",
    "\n",
    "    if trainGen:\n",
    "        optimizerGen.apply_gradients(zip(gradsGen, generator.trainable_weights)) \n",
    "    if trainDis:\n",
    "        optimizerDis.apply_gradients(zip(gradsDis, discriminator.trainable_weights)) \n",
    "\n",
    "    return lossGen, totalLossDis\n",
    "\n",
    "\n",
    "\n",
    "maxStep=0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch+1,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(batchedDataset):\n",
    "\n",
    "        if step%3==0:\n",
    "            lossGen, lossDis = trainStepGen(x_batch_train, trainDis=True)\n",
    "        else:\n",
    "            lossGen, lossDis = trainStepGen(x_batch_train, trainDis=False)\n",
    "\n",
    "        # Log \n",
    "        if step % LOG_INTERVAL == 0:\n",
    "            template = 'Epoch {}/Step {}, Loss Generator: {:.4f}, Loss Discriminator: {:.4f}, Accuracy Dis: {:.4f}'\n",
    "            print(template.format(epoch+1, step, lossGen.numpy(), lossDis.numpy(),  accuracyDis.result().numpy()))\n",
    "            \n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('lossGen', lossGen, step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('lossDis', lossDis, step=maxStep*epoch+step)\n",
    "                tf.summary.scalar('Disc Accuracy', accuracyDis.result().numpy(), step=maxStep*epoch+step)\n",
    "                summary_writer.flush()\n",
    "\n",
    "    maxStep=step\n",
    "\n",
    "    accuracyDis.reset_state()\n",
    "\n",
    "    print(\"Saving images\")\n",
    "    saveImages(generator, epoch)\n",
    "\n",
    "    if saveModel:\n",
    "      generator.save(os.path.join(DATA_PATH, \"model_\"+\"_epoch_{}_valF1Score{:.3f}\".format(epoch, float(val_f1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "testInput = tf.random.uniform((9, 1024))\n",
    "predictions = generator(testInput, training=False)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(np.rint(predictions[i, :, :, :] * 127.5 + 127.5).astype(int))\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
